<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DreamRelation: Bridging Customization and Relation Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://scholar.google.com.hk/citations?user=VpSqhJAAAAAJ&hl=zh-CN">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DreamRelation: Bridging Customization and Relation Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=VpSqhJAAAAAJ&hl=zh-CN">Qingyu Shi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://luqi.info/">Lu Qi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jianzongwu.github.io/">Jianzong Wu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://noyii.github.io/">Jinbin Bai</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ&hl=zh-CN">Yunhai Tong</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>3,5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Peking University,</span>
            <span class="author-block"><sup>2</sup>Insta360 Research,</span>
            <span class="author-block"><sup>3</sup>Nanyang Technological University,</span>
            <span class="author-block"><sup>4</sup>National University of Singapore,</span>
            <span class="author-block"><sup>5</sup>Shanghai AI Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.23280"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.23280"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://shi-qingyu.github.io/RelationBooth/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://shi-qingyu.github.io/RelationBooth/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="Teaser Image" style="width: 100%;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf"> Relation-aware customized image generation, the generated image should strictly keep the relationship and preserve each identity provided by the image prompts.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Customized image generation is essential for creating personalized content based on user prompts, 
            allowing large-scale text-to-image diffusion models to more effectively meet individual needs. 
            However, existing models often neglect the relationships between customized objects in generated images. 
            In contrast, this work addresses this gap by focusing on relation-aware customized image generation, 
            which seeks to preserve the identities from image prompts while maintaining the relationship specified in text prompts. 
            Specifically, we introduce DreamRelation, a framework that disentangles identity and relation learning using a carefully curated dataset. 
            Our training data consists of relation-specific images, independent object images containing identity information, 
            and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges: 
            generating accurate and natural relationships, especially when significant pose adjustments are required, 
            and avoiding object confusion in cases of overlap. 
            First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. 
            Second, we incorporate local features of the image prompts to better distinguish between objects, preventing confusion in overlapping cases. 
            Extensive results on our proposed benchmarks demonstrate the superiority of DreamRelation in generating precise relations while preserving object identities across a diverse set of objects and relationships.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Background. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Background</h2>
        <div class="content has-text-justified">
          <p>
            Customized image generation has made significant progress, driven by advancements in large-scale text-to-image diffusion models like Stable Diffusion and Imagen. 
            These methods have enabled the generation of personalized content by preserving the identity of objects specified by user inputs, 
            proving valuable in areas such as personalized artwork, branding, virtual fashion, social media content, and augmented reality. 
            However, existing techniques primarily focus on individual object customization, often neglecting the importance of relationships between objects in the context of the provided text prompts. 
            Addressing this gap, relation-aware customized image generation emphasizes not only preserving the identity of multiple objects 
            but also accurately capturing the relationships described in the text prompts, presenting new challenges and opportunities in this field.
          </p>
        </div>
      </div>
    </div>
    <!--/ Background. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- 文本部分 -->
    <div class="content">
      <h2 class="title is-3 has-text-centered">Approach</h2>
      <!-- 图像部分 -->
      <div class="has-text-centered" style="margin-bottom: 20px;">
        <img src="static/images/pipeline_new.png" alt="Pipeline" style="max-width: 100%; height: auto; margin-bottom: 20px;">
        <!-- <img src="static/images/data_engine.png" alt="Data Engine" style="max-width: 50%; height: auto;"> -->
      </div>
      <div class="content has-text-justified">
        <p>
          We attribute the failure to two key factors: a lack of relevant data and an ineffective model design. Unlike data
          augmentation techniques such as flipping or rotation, commonly used to create paired training data in object customization methods, our approach requires a triplet
          of images: two image prompts and one target image. The
          image prompts should contain similar objects but exhibit
          distinct poses compared to the target image. To collect
          these triplets, we propose a data engine to curate our finetuning set. We leverage an advanced text-to-image generation model 
          to generate triplets where the same object pair is shared across the images. Through text prompt guidance, 
          the image prompt provides strong identity information. This enables the decoupling of the relationship in the
          target image, which enhances the relation learning.
        </p>
        <p>
          For the model design, we propose DreamRelation, which applies the Low-Rank Adaptation (LoRA) strategy to the text cross-attention layers of existing diffusion models to process user-provided text prompts. 
          In DreamRelation, two key modules are introduced during training to enhance the relation generation in customized generation. 
          First, we introduce a keypoint matching loss (KML) as additional supervision to explicitly encourage the model to
          manipulate object poses, since relationships between objects are closely tied to their poses. Importantly, the KML
          operates on the latent representation rather than the original image space, aligning with the default diffusion loss.
          Second, since relation-aware customization requires local features from image prompts, such as the “hands” features
          for generating “shaking hands”—which are not captured by CLIP's coarse image-level features, we introduce dense fea-
          tures from CLIP. Through partitioning and pooling, we obtain local tokens that contain detail and local information.
          To further enhance the compatibility between dense features and image-level features, we employ a self-distillation
          method to improve their alignment.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <img src="static/images/main_results_multi.png" alt="Results of RelationBooth" style="max-width: 100%; height: auto; margin-bottom: 20px;">
          <img src="static/images/main_results.png" alt="Results of DreamRelation" style="max-width: 100%; height: auto; margin-bottom: 20px;">
          <img src="static/images/multi_comparison.png" alt="Multi Comparison Image" style="max-width: 100%; height: auto; margin-bottom: 20px;">
          <img src="static/images/tab_results.png" alt="Results of RelationBooth" style="max-width: 100%; height: auto; margin-bottom: 20px;">
          <p>
            To comprehensively evaluate relation-aware customized image generation, 
            we developed RelationBench, building on two well-established benchmarks, DreamBench and CustomConcept101. 
            Our methods demonstrated strong performance, with the generated images closely adhering to the relationships 
            specified in the text prompts and maintaining the identities from the image prompts, 
            achieving significant improvements in both visual quality and quantitative results.
          </p>
        </div>
      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Ablation</h2>
        <!-- 图片部分 -->
        <div class="content">
          <!-- 第一张图片单独显示 -->
          <div class="has-text-centered" style="margin-bottom: 20px;">
            <img src="./static/images/ablation_rv.png" alt="Chair TP" style="max-width: 100%; height: auto;">
          </div>
          <!-- 第二组图片并排显示 -->
          <div class="columns is-centered" style="margin-bottom: 20px;">
            <div class="column is-half has-text-centered">
              <img src="./static/images/ablation_vertical.png" alt="Steve" style="max-width: 100%; height: auto;">
            </div>
            <div class="column is-half has-text-centered">
              <img src="./static/images/append_abl.png" alt="Chair TP" style="max-width: 100%; height: auto;">
            </div>
          </div>
        </div>
        <!-- 文本部分 -->
        <div class="content has-text-justified">
          <p>
            In our project, we explored several techniques to enhance relation-aware image generation. 
            First, we experimented with fine-tuning text embeddings in using blank image prompts, 
            but this underperformed due to entanglement of identity and relation information. 
            Next, we observed that including Keypoint Matching Loss (KML) significantly improved image quality and accuracy. 
            Lastly, local token injection was crucial for preventing object confusion. 
            These combined approaches enable high-quality, relation-aware image customization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">For Relation Inversion Task</h2>
        <div class="content has-text-justified">
          <img src="./static/images/relation_inversion.png" alt="Steve" height="max-width: 100%; height: auto; margin-bottom: 20px;">
        </div>
        <div class="content has-text-justified">
          <p>
            Notably, after fine-tuning, our RelationLoRA can also be directly integrated
            into SDXL, effectively enabling the model to address the relation inversion task.
          </p>
        </div>
      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Incorporate with CogVideoX-5b-I2V</h2>
        <div class="content has-text-justified">
          
          <!-- Video Container -->
          <div id="video-carousel" class="video-carousel" style="display: flex; overflow-x: auto; scroll-behavior: smooth; gap: 50px;">
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_dog_is_shaking_hands_with_a_plushie_bear.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_dog_is_fencing_with_a_plushie_panda.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_dog_is_shaking_hands_with_a_plushie_panda.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_person_is_shaking_hands_with_a_dog.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_plushie_cat_is_shaking_hands_with_a_dog.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_cat_is_hugging_with_a_plushie_bear.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <video controls autoplay muted loop playsinline style="max-width: 100%; height: auto;">
              <source src="./static/videos/A_cat_is_shaking_hands_with_a_monster_toy.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>

          </div>
  
        </div>
      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
